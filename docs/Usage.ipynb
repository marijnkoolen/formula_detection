{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e93791c-196d-49a6-9f3b-1fc01db61842",
   "metadata": {},
   "source": [
    "# USage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747bc37-e5f8-41e4-a66e-7b4cb2fcb608",
   "metadata": {},
   "source": [
    "The main class in the [formula_detection](https://pypi.org/project/formula_detection/) package is `FormulaSearch`. This instantiates a searcher that iterates over a list of tokenized texts and identifies candidate formulaic phrases.\n",
    "\n",
    "It builds up frequency lists of unigram and bigram tokens (with skips) and uses these to efficiently filter the search space.\n",
    "\n",
    "## Phrases, tokens and variable terms\n",
    "\n",
    "Phrases are sequences of tokens, which can be contiguous (the tokens appear in a continuous sequence in the source text), but can also contain variable elements (where variable tokens can represent any surface term).\n",
    "\n",
    "## Input texts\n",
    "\n",
    "The `FormulaSearch` class takes as input a set of tokenized texts. In the simplest form, these are lists of tokenized text strings -- that is, a list of a list of token-level strings. For this, you can use any tokenizer to preprocess the texts.\n",
    "\n",
    "Because this package builds on the [fuzzy-search](https://pypi.org/project/fuzzy-search/) pacakge, they can also be texts tokenized by a `fuzzy-search` tokenizer (in that case, the input is a list of `Document` elements or a list of lists of `Token` elements).\n",
    "\n",
    "## Streaming text files as inputs\n",
    "\n",
    "If you have a large amount of text, it can be more (memory) efficient to read the input texts directly from one or more files. This is possible because the `FormulaSearch` class takes any Iterable as input, as long as it iterates over lists of tokens.\n",
    "\n",
    "This notebook shows how to create a simple `Iterable` class that allows you to stream the text from one or more files, apply a tokenizer and pass the resulting lists of tokens to `FormulaSearch` document iterator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa25a4d-0fdf-460a-a4ba-1f99ad9f74e1",
   "metadata": {},
   "source": [
    "# Passing Texts as Tokenized Strings\n",
    "\n",
    "We start with the simplest form: texts as lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0e4097-7362-4ddb-81ec-4e85ac8fdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf008af0-4223-43bf-ad31-704be2491632",
   "metadata": {},
   "source": [
    "To demonstrate the need for tokenization, we pass a single, untokenized sentence string to the searcher. Because the `FormulaSearch` document iterator iterates over the tokens in each document, and due to nature of Python strings, in this case, it iterates over each character in the sentence and treats it as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06bd7482-fbee-4bd4-8b0a-af4157acf31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Iterating over sentences to calculate term frequencies\n",
      "    full collection size (tokens): 62\n",
      "    full lexicon size (types): 22\n",
      "    minimum term frequency: 1\n",
      "    minimum frequency lexicon size: 22\n",
      "WARNING: No value passed for min_cooc_freq, skipping co-occurrence calculations.\n"
     ]
    }
   ],
   "source": [
    "from formula_detection.search import FormulaSearch\n",
    "\n",
    "texts = [\"Yeah, well, you know, that's just like, uh, your opinion, man.\"]\n",
    "\n",
    "formula_search = FormulaSearch(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0df85-b7e9-4436-b178-d8acdaf2842a",
   "metadata": {},
   "source": [
    "The `FormulaSearch` instance indexes each token (mapping it to a numeric identifier for efficiency) and keeps track of the full vocabulary and the frequency of each token type in the vocabulary.\n",
    "\n",
    "We can see the result by inspecting the term-to-identifier dictionary of the full vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "758a82e5-d2ff-4f2e-98bb-81bfe8d69390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Y': 0,\n",
       " 'e': 1,\n",
       " 'a': 2,\n",
       " 'h': 3,\n",
       " ',': 4,\n",
       " ' ': 5,\n",
       " 'w': 6,\n",
       " 'l': 7,\n",
       " 'y': 8,\n",
       " 'o': 9,\n",
       " 'u': 10,\n",
       " 'k': 11,\n",
       " 'n': 12,\n",
       " 't': 13,\n",
       " \"'\": 14,\n",
       " 's': 15,\n",
       " 'j': 16,\n",
       " 'i': 17,\n",
       " 'r': 18,\n",
       " 'p': 19,\n",
       " 'm': 20,\n",
       " '.': 21}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula_search.full_vocab.term_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e641fb6-e128-43c9-8354-2a4ae15ee314",
   "metadata": {},
   "source": [
    "If we instead tokenize the sentence by splitting on whitespace, the index consists of a list of words (with puncuation attached, because we using a simplistic tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696a2ad7-fed0-4ff6-833c-75b427b14102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Iterating over sentences to calculate term frequencies\n",
      "    full collection size (tokens): 11\n",
      "    full lexicon size (types): 11\n",
      "    minimum term frequency: 1\n",
      "    minimum frequency lexicon size: 11\n",
      "WARNING: No value passed for min_cooc_freq, skipping co-occurrence calculations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Yeah,': 0,\n",
       " 'well,': 1,\n",
       " 'you': 2,\n",
       " 'know,': 3,\n",
       " \"that's\": 4,\n",
       " 'just': 5,\n",
       " 'like,': 6,\n",
       " 'uh,': 7,\n",
       " 'your': 8,\n",
       " 'opinion,': 9,\n",
       " 'man.': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = [text.split(' ') for text in texts]\n",
    "\n",
    "formula_search = FormulaSearch(tokenized_texts)\n",
    "\n",
    "formula_search.full_vocab.term_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8bb10-a393-4feb-9899-eedb9742a36e",
   "metadata": {},
   "source": [
    "As you can see, the tokenized terms are indexed as is, case-sensitive and including punctuation. You can use any tokenizer you want to process the texts as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb9ae7-e618-45e2-93d5-1e0e4a242ea7",
   "metadata": {},
   "source": [
    "## Streaming Texts from Disk\n",
    "\n",
    "In cases where you have large amounts of text that you can't or don't want to completely read in memory, you can create a Python `Iterable` that can stream the texts from desk while the formula searcher is iterating over the texts. A good explanation and example is given in the [Gensim documentation](https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-one-document-at-a-time). \n",
    "\n",
    "You can pass the filenames of one or more text files to a class that can read their contents and has an `__iter__` method that yields tokenized documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003cbea3-4aa9-49f9-8100-b14ce7e7a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yeah,', 'well,', 'you', 'know,', \"that's\", 'just', 'like,', 'uh,', 'your', 'opinion,', 'man.']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class TextIterable:\n",
    "\n",
    "    def __init__(self, temp_files):\n",
    "        self.temp_files = temp_files if isinstance(temp_files, list) else [temp_files]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for temp_file in self.temp_files:\n",
    "            with open(temp_file, 'rt') as fh:\n",
    "                for line in fh:\n",
    "                    tokens = line.strip('\\n').split(' ')\n",
    "                    yield tokens\n",
    "\n",
    "\n",
    "# First, write the texts (containing only the example sentence) to file\n",
    "temp_file = 'temp.txt'\n",
    "with open(temp_file, 'wt') as fh:\n",
    "    for text in texts:\n",
    "        fh.write(f\"{text}\\n\")\n",
    "\n",
    "# Create an iterable and pass the text filename\n",
    "text_iter = TextIterable(temp_file)\n",
    "\n",
    "# iterate over the documents in the file and pass each document as a list of tokens\n",
    "for tokens in text_iter:\n",
    "    print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de6c4a-7cd4-41d3-921d-f92233ac3b63",
   "metadata": {},
   "source": [
    "Now you can pass the iterable to the formula searcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a31228-2db2-4907-aa6e-c4522325ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Iterating over sentences to calculate term frequencies\n",
      "    full collection size (tokens): 11\n",
      "    full lexicon size (types): 11\n",
      "    minimum term frequency: 1\n",
      "    minimum frequency lexicon size: 11\n",
      "WARNING: No value passed for min_cooc_freq, skipping co-occurrence calculations.\n"
     ]
    }
   ],
   "source": [
    "formula_search = FormulaSearch(text_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40318699-a2cf-402d-9d9c-62097711891e",
   "metadata": {},
   "source": [
    "Because we've created a temporary file, it's good to clean up after ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147fd032-f828-4e46-a2a8-ee13cbd23dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.unlink(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad64ad5-822c-4d5d-a4d4-91d860709a74",
   "metadata": {},
   "source": [
    "## A more serious example\n",
    "\n",
    "Let's use a longer text with some repetition to see the formula searcher in action. The code below downloads a few texts from the excellent [Project Gutenberg](https://gutenberg.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74f11f75-c2cb-449a-b270-3f94f3623832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "text_files = [\n",
    "    {\n",
    "        'name': 'Church of Scotland. General Assembly', \n",
    "        'url': 'https://www.gutenberg.org/cache/epub/28957/pg28957.txt',\n",
    "        'file': 'church_acts.txt'\n",
    "    },\n",
    "    {\n",
    "        'name': 'The Bible, Douay-Rheims, Complete', \n",
    "        'url': 'https://www.gutenberg.org/cache/epub/1581/pg1581.txt',\n",
    "        'file': 'bible.txt'\n",
    "    },\n",
    "    {\n",
    "        'name': 'The Gilgamesh Epic', \n",
    "        'url': 'https://www.gutenberg.org/cache/epub/11000/pg11000.txt',\n",
    "        'file': 'gilgamesh.txt'\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for text_file in text_files:\n",
    "    if os.path.exists(text_file['file']) is False:\n",
    "        response = requests.get(text_file['url'])\n",
    "        if response.status_code == 200:\n",
    "            with open(text_file['file'], 'wt') as fh:\n",
    "                fh.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53e0e6ff-4ffa-438a-9032-0ded63b3ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Abridgment of the Debates of Congress, from 1789 to 1856, Vol. 1-4 (of 16)'\n",
    "text_urls = [\n",
    "    'https://www.gutenberg.org/cache/epub/40499/pg40499.txt',\n",
    "    'https://www.gutenberg.org/cache/epub/40851/pg40851.txt',\n",
    "    'https://www.gutenberg.org/cache/epub/54345/pg54345.txt',\n",
    "    'https://www.gutenberg.org/cache/epub/47289/pg47289.txt',\n",
    "]\n",
    "\n",
    "for ui, url in enumerate(text_urls):\n",
    "    text_file = f\"debates_of_congress-vol_{ui+1}.txt\"\n",
    "    if os.path.exists(text_file) is False:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(text_file, 'wt') as fh:\n",
    "                fh.write(response.text)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa0f9c7-3e22-4e78-8360-4af6669268c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['debates_of_congress-vol_1.txt',\n",
       " 'debates_of_congress-vol_3.txt',\n",
       " 'debates_of_congress-vol_2.txt',\n",
       " 'debates_of_congress-vol_4.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "text_files = glob.glob('debates_of_congress-vol_*.txt')\n",
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39096f3a-30d4-4465-976a-efaad566797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents: 325,831\n",
      "number of tokens: 3,465,969\n"
     ]
    }
   ],
   "source": [
    "text_iter = TextIterable(text_files)\n",
    "docs = [tokens for tokens in text_iter]\n",
    "print(f\"number of documents: {len(docs):,}\\nnumber of tokens: {sum([len(doc) for doc in docs]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a859b78-bfc9-4e99-9350-19738aef97f7",
   "metadata": {},
   "source": [
    "These four volumes together contain 3.5 million words across 325,831 lines.\n",
    "\n",
    "If we pass these to the formula searcher, we get a larger vocabulary and term frequency index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a57b2a1-b853-4fb2-a8fa-ec9e3a445343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Iterating over sentences to calculate term frequencies\n",
      "    full collection size (tokens): 3,465,969\n",
      "    full lexicon size (types): 76,530\n",
      "    minimum term frequency: 1\n",
      "    minimum frequency lexicon size: 76,530\n",
      "WARNING: No value passed for min_cooc_freq, skipping co-occurrence calculations.\n"
     ]
    }
   ],
   "source": [
    "formula_search = FormulaSearch(text_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abad767-b790-440e-a890-fa0bbbeb7321",
   "metadata": {},
   "source": [
    "The vocabulary consists of 76,530 distinct terms (token types), and of course, the least frequency types occur only once. For finding formulaic phrases, we're interested in words combinations that occur frequently, so low-frequency terms can safely be removed, both to make the term frequency and co-occurrence frequency indexes smaller and to make the search space smaller. With the `min_term_freq` parameter you can control the minimum frequency for including terms in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30f4f330-7223-44af-86ca-62cb14682a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Iterating over sentences to calculate term frequencies\n",
      "    full collection size (tokens): 3,465,969\n",
      "    full lexicon size (types): 76,530\n",
      "    minimum term frequency: 10\n",
      "    minimum frequency lexicon size: 15,196\n",
      "WARNING: No value passed for min_cooc_freq, skipping co-occurrence calculations.\n"
     ]
    }
   ],
   "source": [
    "formula_search = FormulaSearch(text_iter, min_term_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297b2b1-173c-412b-8eca-dd921625d764",
   "metadata": {},
   "source": [
    "The full lexicon (`full_vocab`) contains 76,530 distinct terms while the minimum frequency lexicon (`min_freq_vocab`) contains only 15,196 terms. Only the terms in the minimum frequency lexicon are considered as elements of formulaic phrases.\n",
    "\n",
    "You can check the terms contained in the frequency index (a Python `Counter` object) by mapping the term IDs to their respective terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa18a00c-f1ad-4405-9c62-b589757439ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 385093\n",
      "the 244216\n",
      "of 146227\n",
      "to 113052\n",
      "and 73445\n",
      "a 50936\n",
      "in 49720\n",
      "that 45814\n",
      "be 37605\n",
      "it 29161\n"
     ]
    }
   ],
   "source": [
    "for term_id, freq in formula_search.term_freq.most_common(10):\n",
    "    print(formula_search.id2term(term_id), freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154580f7-81ec-4ac0-8527-159bcf06f2e8",
   "metadata": {},
   "source": [
    "The most common terms tend to be stopwords, which do not get removed from the vocabulary, because they are part of formulaic phrases.\n",
    "\n",
    "As you can see, the most common term is the empty string (''). This simplistic tokenizer has a few issues that are can be solved by using a more sophisicated tokenizer (see below).\n",
    "\n",
    "Next you can calculate the co-occurrence frequency of a term and it's neighbouring terms using the `skip_size` parameter (which defaults to `skip_size=4`), meaning, the co-occurrence of the term and up to `skip_size+1` neighbouring terms (only if both terms are in the minimum frequency lexicon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09a86c78-0c99-49ea-b60d-4b3b17191d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Iterating over sentences to calculate the co-occurrence frequencies\n",
      "docs: 325,831\tnum_words: 3,465,969\tnum_coocs: 12,891,052\tnum distinct coocs: 2,938,969\n",
      "    co-occurrence index size: 2,938,969\n"
     ]
    }
   ],
   "source": [
    "formula_search.calculate_co_occurrence_frequencies(skip_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4042beb-f05f-4530-914d-09b951f515e6",
   "metadata": {},
   "source": [
    "The co-occurrence index contains almost 3 million bigrams. Again, the most common ones tend to be combinations of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "133465be-19b1-425f-9ca7-b4955f960e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ''] 850403\n",
      "['the', 'of'] 77174\n",
      "['of', 'the'] 72750\n",
      "['', 'the'] 59062\n",
      "['the', 'the'] 54084\n",
      "['to', 'the'] 48665\n",
      "['', 'of'] 40906\n",
      "['', 'to'] 25126\n",
      "['in', 'the'] 24017\n",
      "['and', 'the'] 22086\n"
     ]
    }
   ],
   "source": [
    "for term_ids, freq in formula_search.cooc_freq.most_common(10):\n",
    "    terms = [formula_search.id2term(term_id) for term_id in term_ids]\n",
    "    print(terms, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806620c-7e05-4a33-b49e-0039db744242",
   "metadata": {},
   "source": [
    "Finally, we get to the point where we can ask the formula searcher to find candidate formulaic phrases using `extract_phrases`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72f40aa6-5dd4-41a6-973b-b58b7d64d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum co-occurrence frequency: 5\n",
      "phrase: Project Gutenberg eBook of Abridgment\n",
      "phrase: Gutenberg eBook of Abridgment of\n",
      "phrase: eBook of Abridgment of the\n",
      "phrase: of Abridgment of the Debates\n",
      "phrase: Abridgment of the Debates of\n",
      "phrase: of the Debates of Congress,\n",
      "phrase: the Debates of Congress, from\n",
      "phrase: Debates of Congress, from 1789\n",
      "phrase: of Congress, from 1789 to\n",
      "phrase: Congress, from 1789 to 1856,\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "extractor = (formula_search\n",
    "             .extract_phrases(phrase_type='sub_phrases', max_phrase_length=5, min_cooc_freq=5))\n",
    "\n",
    "for ti, candidate_pm in enumerate(extractor):\n",
    "    print('phrase:', candidate_pm.phrase)\n",
    "    if (ti+1) == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74686b93-1551-4e25-8b02-90cb8feb7c5d",
   "metadata": {},
   "source": [
    "Here you see phrases from the standard Project Gutenberg preamble. This is not particularly interesting. You can get more insight in the most common phrases by counting their frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a93306e8-e745-4ad8-a0c1-6b3f4148d3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum co-occurrence frequency: 10\n"
     ]
    }
   ],
   "source": [
    "extractor = (formula_search\n",
    "             .extract_phrases_from_docs(phrase_type='sub_phrases', max_phrase_length=5, min_cooc_freq=10))\n",
    "\n",
    "sub_phrase_freq = Counter()\n",
    "for ti, candidate_pm in enumerate(extractor):\n",
    "    sub_phrase_freq.update([candidate_pm.phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab9a676f-9d52-4e8d-ade9-15980789b590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('    ', 103719),\n",
       " ('    the', 3012),\n",
       " ('    of', 1521),\n",
       " ('    and', 1014),\n",
       " ('    to', 962),\n",
       " ('    on', 710),\n",
       " ('   of the', 540),\n",
       " ('the President of the United', 512),\n",
       " ('    The', 498),\n",
       " ('    in', 487),\n",
       " ('  |  ', 457),\n",
       " (' |   ', 431),\n",
       " ('   | ', 428),\n",
       " ('the PRESIDENT OF THE UNITED', 370),\n",
       " ('    that', 351),\n",
       " ('into a Committee of the', 350),\n",
       " ('President of the United States', 343),\n",
       " ('    it', 337),\n",
       " ('Committee of the Whole on', 321),\n",
       " ('   on the', 314)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6d12e-ffbf-419f-9356-7adb65e9a574",
   "metadata": {},
   "source": [
    "The majority of frequent phrases consist of one or more empty strings. It's time to improve the tokenizer, so you get a better idea of what candidate formulaic phrases can be identified with a more informed tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cf4ea-1872-4f3d-a85c-3718e65293d6",
   "metadata": {},
   "source": [
    "## Tokenizers and the impact of normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3dec95-4328-4b51-bc2c-13493104dd5f",
   "metadata": {},
   "source": [
    "We've seen a number of issues when using a simplistic tokenizer:\n",
    "\n",
    "1. The tokenized strings contain many empty strings ('') as tokens.\n",
    "2. Many tokens have puncuation attached to words, which makes them distinct terms (token types) from the same words without punctuation.\n",
    "3. The tokenizer is case-sensitive, which may be a hurdle if formulaic phrases have (unhelpful) variation in the use of case.\n",
    "\n",
    "A slightly more sophisticated tokenizer is provided by the [fuzzy-search]() package, which is one of the dependencies of the `formula_detection` package, so is already installed if you've used a package manager to install `formula_detection`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20f8d986-d3de-4fb0-92cb-66a44721da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Abridgment of the Debates of Congress, from 1789 to 1856, Vol. 1 (of 16)\n",
      "\n",
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'abridgment', 'of', 'the', 'debates', 'of', 'congress', 'from', '1789', 'to', '1856', 'vol', '1', 'of', '16']\n",
      "\n",
      "    \n",
      "\n",
      "[]\n",
      "\n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "\n",
      "['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and']\n",
      "\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "\n",
      "['most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions']\n",
      "\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "\n",
      "['whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fuzzy_search.tokenization.token import Tokenizer\n",
    "\n",
    "# ignorecase transforms everything to lowercase\n",
    "# remove_punctuation uses re.split(r'\\W+', token) to strip non-alphanumeric chars\n",
    "tokenizer = Tokenizer(ignorecase=True, remove_punctuation=True)\n",
    "\n",
    "with open(text_files[0], 'rt') as fh:\n",
    "    lines = [line for line in fh]\n",
    "    for line in lines[:5]:\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        print(line)\n",
    "        print(tokens)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09dafd-21fc-4491-a4e0-2e1d179d988e",
   "metadata": {},
   "source": [
    "We add one more improvement to the text iteratable. The Gutenberg Project files use a maximum line width for ease of reading and break sentences over these line widths. The boundaries between paragraphs are indicated by a double newline. \n",
    "\n",
    "So by treating each line as a document, the words in two consecutive lines may belong to the same sentence and paragraph, but are treated as belonging to different documents. In other words, the line as document treatment misses many term co-occurrences.\n",
    "\n",
    "We update the text iterable by first generating paragraphs from the lines, then tokenizing the paragraphs. \n",
    "\n",
    "(**Note** if you care about not crossing sentence boundaries, you could of course use a sentence tokenizer before using a word tokenizer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c9c5f89-68a6-4588-bb82-7219eb8b2661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'abridgment', 'of', 'the', 'debates', 'of', 'congress', 'from', '1789', 'to', '1856', 'vol', '1', 'of', '16', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook']\n"
     ]
    }
   ],
   "source": [
    "from demo_helper import lines_to_paras\n",
    "\n",
    "\n",
    "class TokenizedTextIterable:\n",
    "\n",
    "    def __init__(self, text_files, tokenizer = None):\n",
    "        self.text_files = text_files if isinstance(text_files, list) else [text_files]\n",
    "        self.tokenize = tokenizer.tokenize if tokenizer is not None else lambda line: line.strip('\\n').split(' ')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text_file in self.text_files:\n",
    "            with open(text_file, 'rt') as fh:\n",
    "                lines = [line for line in fh]\n",
    "                for para in lines_to_paras(lines):\n",
    "                    yield self.tokenize(para)\n",
    "            \n",
    "    \n",
    "text_iter = TokenizedTextIterable(text_files, tokenizer)\n",
    "#text_iter = TokenizedTextIterable('gutenberg_intro.txt', tokenizer)\n",
    "#text_iter = TokenizedTextIterable('gilgamesh.txt', tokenizer)\n",
    "\n",
    "for tokens in text_iter:\n",
    "    print(tokens)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae5d1d-6f08-4197-b4c9-c54bf3479181",
   "metadata": {},
   "source": [
    "Now the first document corresponds to the first paragraph in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88814bfa-9aef-4b68-870a-767fb65503a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Iterating over sentences to calculate term frequencies\n",
      "    full collection size (tokens): 3,102,812\n",
      "    full lexicon size (types): 26,514\n",
      "    minimum term frequency: 10\n",
      "    minimum frequency lexicon size: 9,862\n",
      "2. Iterating over sentences to calculate the co-occurrence frequencies\n",
      "docs: 39,245\tnum_words: 3,102,812\tnum_coocs: 14,566,150\tnum distinct coocs: 2,227,284\n",
      "    co-occurrence index size: 2,227,284\n"
     ]
    }
   ],
   "source": [
    "formula_search = FormulaSearch(text_iter, min_term_freq=10, min_cooc_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe9e4c-d365-479a-9a90-0f7bfc8b3919",
   "metadata": {},
   "source": [
    "The removal of punctuation and lowercasing of all tokens has resulted in a much smaller full lexicon (26,514 instead of 76,530 terms) and also a smaller minimum frequency lexicon (9,862 instead of 15,196 terms). \n",
    "\n",
    "However, by concatenating lines to paragraphs, the total number of bigram co-occurrences has gone up from 12,891,052 to 14,566,150. But because of the smaller, normalised lexicon, the distinct number of co-occurring bigrams dropped from 2,938,969 to 2,227,284.\n",
    "\n",
    "Next, we can extract candidate formulaic phrases again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "703e9392-2712-4218-8e16-ce3319f249dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum co-occurrence frequency: 10\n"
     ]
    }
   ],
   "source": [
    "extractor = (formula_search\n",
    "             .extract_phrases(phrase_type='sub_phrases', max_phrase_length=5, min_cooc_freq=10))\n",
    "\n",
    "sub_phrase_freq = Counter()\n",
    "for ti, candidate_pm in enumerate(extractor):\n",
    "    sub_phrase_freq.update([candidate_pm.phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f26af9fd-08fc-4785-a702-c8f8ec6b7914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('president of the united states', 1572),\n",
       " ('the president of the united', 1325),\n",
       " ('of the united states and', 729),\n",
       " ('a committee of the whole', 698),\n",
       " ('committee of the whole on', 464),\n",
       " ('into a committee of the', 420),\n",
       " ('of the whole on the', 391),\n",
       " ('of the united states to', 386),\n",
       " ('on the part of the', 343),\n",
       " ('itself into a committee of', 317),\n",
       " ('of the house of representatives', 311),\n",
       " ('resolved itself into a committee', 304),\n",
       " ('the secretary of the treasury', 274),\n",
       " ('the committee of the whole', 272),\n",
       " ('the report of the committee', 256),\n",
       " ('committee of the whole house', 254),\n",
       " ('government of the united states', 252),\n",
       " ('on the president of the', 250),\n",
       " ('constitution of the united states', 246),\n",
       " ('the constitution of the united', 238)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80534ea0-cf94-4036-a73e-a2c927d824a5",
   "metadata": {},
   "source": [
    "## Allowing variable terms\n",
    "\n",
    "So far, we've only extracted continuous phrases (only terms that are directly adjacent to each other), but formulaic phrases can have contain variable elements. These are typically the names of persons, locations, dates, or words that optional and only occur in a fraction of the uses of a formulaic phrase. The use of skips allows the searcher to identify frequently occurring phrases with such variable elements. To show that a phrase contains a variable element, the variable token is replaced by `<VAR>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fb865c0-d756-4817-acd6-1e8ea300f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum co-occurrence frequency: 10\n"
     ]
    }
   ],
   "source": [
    "extractor = (formula_search\n",
    "             .extract_phrases(phrase_type='sub_phrases', max_phrase_length=8,\n",
    "                              max_variables=2, min_cooc_freq=10))\n",
    "\n",
    "sub_phrase_freq = Counter()\n",
    "for ti, candidate_pm in enumerate(extractor):\n",
    "    sub_phrase_freq.update([candidate_pm.phrase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3740dc1-b6ca-4e59-a810-ed7ff67f8e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on the execution of the british treaty <VAR> 16\n",
      "rights of the house relative to treaties <VAR> 16\n",
      "<VAR> appointed a senator by the legislature of 15\n",
      "on an additional military force <VAR> _see index_ 9\n",
      "of the house relative to treaties <VAR> on 8\n",
      "the house relative to treaties <VAR> on the 8\n",
      "<VAR> not protected by u s copyright law 8\n",
      "prosecution on the trial of judge chase <VAR> 8\n",
      "defence on the trial of judge chase <VAR> 8\n",
      "on jurisdiction over the district of columbia <VAR> 8\n",
      "form a more perfect union establish justice <VAR> 7\n",
      "a more perfect union establish justice <VAR> domestic 7\n",
      "more perfect union establish justice <VAR> domestic tranquillity 7\n",
      "perfect union establish justice <VAR> domestic tranquillity provide 7\n",
      "union establish justice <VAR> domestic tranquillity provide for 7\n",
      "establish justice <VAR> domestic tranquillity provide for the 7\n",
      "justice <VAR> domestic tranquillity provide for the common 7\n",
      "<VAR> domestic tranquillity provide for the common defence 7\n",
      "<VAR> on the execution of the british treaty 7\n",
      "commercial intercourse with france and great britain <VAR> 7\n",
      "house relative to treaties <VAR> on the execution 6\n",
      "relative to treaties <VAR> on the execution of 6\n",
      "to treaties <VAR> on the execution of the 6\n",
      "treaties <VAR> on the execution of the british 6\n",
      "officers and crew of the united states <VAR> 6\n",
      "to such english authorities as they believed <VAR> 6\n",
      "inquiry into the conduct of gen wilkinson <VAR> 6\n",
      "<VAR> appointed a senator by the state of 6\n",
      "on exempting bank notes from stamp duty <VAR> 6\n",
      "non intercourse with great britain and france <VAR> 6\n",
      "the town of <VAR> in the state of 5\n",
      "zephaniah swift silas <VAR> george thatcher uriah tracy 5\n",
      "<VAR> on the right to indian lands within 5\n",
      "the execution of the british treaty <VAR> on 5\n",
      "execution of the british treaty <VAR> on the 5\n",
      "<VAR> on a salary for members of congress 5\n",
      "and crew of the united states <VAR> <VAR> 5\n",
      "the bill <VAR> from postage all letters and 5\n",
      "bill <VAR> from postage all letters and packets 5\n",
      "<VAR> from postage all letters and packets to 5\n",
      "such english authorities as they believed <VAR> or 5\n",
      "english authorities as they believed <VAR> or from 5\n",
      "authorities as they believed <VAR> or from citing 5\n",
      "as they believed <VAR> or from citing certain 5\n",
      "they believed <VAR> or from citing certain statutes 5\n",
      "believed <VAR> or from citing certain statutes of 5\n",
      "<VAR> or from citing certain statutes of the 5\n",
      "a dam or <VAR> from mason s island 5\n",
      "dam or <VAR> from mason s island to 5\n",
      "or <VAR> from mason s island to the 5\n",
      "<VAR> from mason s island to the western 5\n",
      "president of the united states before he <VAR> 5\n",
      "of the united states before he <VAR> on 5\n",
      "the united states before he <VAR> on the 5\n",
      "united states before he <VAR> on the execution 5\n",
      "states before he <VAR> on the execution of 5\n",
      "before he <VAR> on the execution of his 5\n",
      "he <VAR> on the execution of his office 5\n",
      "<VAR> on the execution of his office on 5\n"
     ]
    }
   ],
   "source": [
    "for phrase, freq in sub_phrase_freq.most_common(5000):\n",
    "    if '<VAR>' not in phrase:\n",
    "        continue\n",
    "    print(phrase, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23935ddd-e93b-40bd-aace-58ebf2e06509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
